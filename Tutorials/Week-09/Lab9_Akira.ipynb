{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "- Dataset: [Pima Indian Diabetes Dataset](http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.names)\n",
    "- The Diabetes dataset records measurements about several hundred patients with an indication of whether or not they tested positive for diabetes (the class label).  \n",
    "- The classification problem is to predict whether a patient will test positive for diabetes given some other measurements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "- (as briefly mentioned in last weeks' tute)\n",
    "- We want to split our dataset into train and test to \"train\" our model (usually finding the coefficients or optimal parameters) before using the \"test\" set to **evaluate** our model.\n",
    "- A common split is to have 80% on training and 20% on test.\n",
    "- More advanced techniques include $k$-fold Cross Validation (CV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('pima-indians-diabetes.csv', encoding='ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (get all columns except the last one which is our label has_diabetes)\n",
    "X = df[df.columns[:-1]]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class label\n",
    "y = df['has_diabetes']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a train test split using 80% for train and 20% for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You should now note that the index column is in some arbitrary order (hence randomly chosen split).\n",
    "- As you can see, some of the attributes are disproportionate to other other attributes (i.e `serum-insulin` has value `370` whilst `pedigree-function` seems to be far smaller).\n",
    "- To combat this, we should normalise (or standardise) our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "- With few exceptions, many Machine Learning (ML) algorithms generally don't preform well when the input data consists of continuous attributes with varying scales.\n",
    "- The two main ways to feature scale are using **Min-Max Scaling** (normalization) or **Standardisation**.\n",
    "    - **Min-Max Scaling:** Data is shifted and scaled such that they now range between 0 and 1.\n",
    "    - **Standardisation:** Data has resulting distribution of 0 mean and unit variance (in other words, variance = 1).\n",
    "    \n",
    "**IMPORTANT:** With any transformation, it is imperative that you fit the scaler to the **training data only** and NEVER to the full dataset. Once you fit your scaler to the training data, then you should use that to trasnform your test.\n",
    "\n",
    "For this example, we will standardise our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# notice here that I fit it to my train\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# and using this \"fit\", I transform both my train and test USING the fitted train scaler\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-Nearest Neighbour (kNN)\n",
    "- A supervised clustering algorithm similar to $k$-means.\n",
    "- Rather than iteratively update the centroids in an unsupervised manner (recall $k$-means does not care about labels), $k$-NN aims to classify new data *given* the label of the existing $k$ nearest points.\n",
    "\n",
    "\n",
    "As such, $k$-means and $k$-NN are used for different problems:\n",
    "- $k$-means clustering is unsupervised that looks to gather and group data into $k$ number of clusters by looking at the data points only;\n",
    "- whilst $k$-NN is a supervised classifier that will classify new data points according to the $k$ nearest data points (that were given during training). \n",
    "- I like to think of $k$-NN as a majority vote, if you have $k$=5 and your new data point has 2 $\\times$ `True` and 3 $\\times$ `False`, then your new data point will be classified as `False`.\n",
    "\n",
    "Hopefully it's intuitive, but to avoid ties in majority votes it's best to use **odd values of $k$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNN(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of code, predicting the class label using $k$-NN is as simple as using `predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate our performance, we can use the Classification Accuracy.\n",
    "- This is literally just the percentage of correct divided by total number of predictions.\n",
    "\n",
    "There are better metrics such as Precision, Recall, and F1-Score as well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Here's a scatter plot that shows varying values of $k$ vs the Classification Accuracy.\n",
    "- What seems to be the best value of $k$ for this dataset?\n",
    "- Is this $k$ always the best for any dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = {}\n",
    "\n",
    "krange = range(1, 201, 2)\n",
    "\n",
    "for k in krange:\n",
    "    knn = KNN(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = knn.predict(X_test)\n",
    "    results[k] = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "plt.plot(krange, results.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(results.items(), key=lambda x: -x[1])[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "- Decision Tree Classifiers are (imho) one of the most useful alternatives to regression and is a great way of comparing different approaches and predictions.\n",
    "- Although trees usually use Gini as a split criterion, we will use the more familiar **entropy** for this example.\n",
    "- Additionally, we'll enforce a \"max depth\" of 3, which controls how \"deep\" or \"big\" a tree can be. This is because we want to visualise the tree itself afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "\n",
    "dt = DT(criterion=\"entropy\", random_state=42, max_depth=3)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dt.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL\n",
    "- Some optional code to \"visualise\" the decision tree\n",
    "\n",
    "WSL2 (Linux):\n",
    "```python\n",
    "pip3 install graphviz\n",
    "sudo apt-get install graphviz\n",
    "```\n",
    "\n",
    "Conda Terminal:\n",
    "```\n",
    "conda install -c conda-forge python-graphviz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(dt, out_file=\"mytree.dot\", \n",
    "                feature_names=X.columns, \n",
    "                filled=True, rounded=True)\n",
    "\n",
    "with open(\"./mytree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "    \n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Here's a scatter plot that shows varying values of the DT given a higher percentage of training data size.\n",
    "- What seems to be the best size of training data?\n",
    "- Is it a good idea to have a training set that big?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_sizes = range(1, 101, 5)\n",
    "results = {}\n",
    "\n",
    "for split in split_sizes:\n",
    "    X1, X2, y_train, y_test = train_test_split(X, y, train_size = split/100, \n",
    "                                      test_size=(1 - split/100), random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler().fit(X1)\n",
    "    X_train = scaler.transform(X1)\n",
    "    X_test = scaler.transform(X2)\n",
    "    \n",
    "    dt.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = dt.predict(X_test)\n",
    "    results[split] = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "plt.plot(split_sizes, results.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "- Refer to Part 2 of Week 8 lab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
