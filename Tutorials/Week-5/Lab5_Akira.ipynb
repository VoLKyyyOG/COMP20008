{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elements Of Data Processing (2021S1) - Week 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Folding\n",
    "- Case folding removes all case distinctions present in a string (i.e lower and upper cases are matched regardless). \n",
    "- It is used for caseless matching when it the text input isn't always guaranteed to have the correct grammar. \n",
    "- Essentially, casefolding is a more aggressive version of the `str.lower()` method that is designed to take into account *much more* unique Unicode characters and make them more comparable.\n",
    "- You can use `str.lower()` when your text field is purely ASCII Text, but you should use `str.casefold()` when working with Unicode text or user input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Exercise 1 </span>\n",
    "\n",
    "Use appropriate functions to covert `\"Whereof one cannot speak, thereof one must be silent.\"` into:\n",
    "- Lower case.\n",
    "- Upper case.\n",
    "- Casefold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Whereof one cannot speak, thereof one must be silent.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing (NLP)\n",
    "- Preprocessing steps for NLP can be done using the `nltk` library.\n",
    "- Provides useful functions for tokenizing, stemming, lemmatizing, and vectorizing text fields.\n",
    "- We don't always need to remove punctuation - sometimes you want to keep the natural language features to help split apart [contractions](https://www.thoughtco.com/contractions-commonly-used-informal-english-1692651).\n",
    "\n",
    "The example below parses the `speech` string and outputs a frequency dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = \"\"\"Four score and seven years ago our fathers brought forth on this continent, \n",
    "a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. \n",
    "Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, \n",
    "can long endure. We are met on a great battle-field of that war. \n",
    "We have come to dedicate a portion of that field, as a final resting place for those who here \n",
    "gave their lives that that nation might live. It is altogether fitting and proper that we should do this. \n",
    "But, in a larger sense, we can not dedicate -- we can not consecrate -- we can not hallow -- this ground. \n",
    "The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. \n",
    "The world will little note, nor long remember what we say here, but it can never forget what they did here.\n",
    "It is for us the living, rather, to be dedicated here to the unfinished work which they who \n",
    "fought here have thus far so nobly advanced. It is rather for us to be here dedicated to the \n",
    "great task remaining before us -- that from these honored dead we take increased devotion to \n",
    "that cause for which they gave the last full measure of devotion -- that we here highly resolve \n",
    "that these dead shall not have died in vain -- that this nation, under God, shall have a new birth \n",
    "of freedom -- and that government of the people, by the people, for the people, shall not perish from the earth.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize words - similar to str.split()\n",
    "words = nltk.word_tokenize(speech)\n",
    "\n",
    "# create a set of stopwords (i.e and, or, is, it, etc)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# initialise the porter stemmer function\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words = [ps.stem(word) for word in words if word not in stop_words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq_stem = Counter(stemmed_words)\n",
    "for word, freq in sorted(freq_stem.items(), key=lambda x: -x[1]):\n",
    "    print(word, freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Exercise 2 </span>\n",
    "\n",
    "- Modify the example above to use a `WordNet` Lemmatizer instead of a Porter Stemmer.\n",
    "- What are the differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# tokenize words - similar to str.split()\n",
    "words = nltk.word_tokenize(speech)\n",
    "\n",
    "# create a set of stopwords (i.e and, or, is, it, etc)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# add code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset: Smokers\n",
    "In the first twenty rows, there are seven errors that all fall into one of the following categories:\n",
    "- Semantic Errors\n",
    "- Range Errors\n",
    "- Format Errors\n",
    "\n",
    "Identify the errors and what category they fall into. \n",
    "- Where possible, fix the errors manually and save the new spreadsheet as `smoking-info-corrected.csv`\n",
    "- Suggest how you would write a program to detect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "df = pd.read_csv('smoking_data_us_1995_2010.csv')\n",
    "display(df.tail())\n",
    "display(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of a percentages are at most 7 characters (XXX.XX%)\n",
    "DECIMAL_LENGTH = 6\n",
    "\n",
    "for col in df.columns[2:]:\n",
    "    print(df[col].apply(lambda x: float(x.rstrip('%')) if len(x) <= DECIMAL_LENGTH else print(x)).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2number import w2n\n",
    "import re\n",
    "\n",
    "word_num = 'twenty-three point 6 percent'\n",
    "\n",
    "# strip percent\n",
    "word_num = re.sub(r'(\\spercent)?', r'', word_num)\n",
    "print(word_num)\n",
    "\n",
    "w2n.word_to_num(word_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As you can see, although libraries can help, they aren't always perfect.\n",
    "- Part of your work may be to discuss with the client and Business Analysts on fixing these issues manually..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Exercise 3 </span>\n",
    "\n",
    "Write python code for the following tasks:\n",
    "- Import your file `smoking_data_us_1995_2010_corrected.csv` into a pandas data frame\n",
    "- Remove the percentage symbols from the data. \n",
    "- For removing/replacing characters see [here](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html)\n",
    "- After the removals, convert all the strings to numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('smoking_data_us_1995_2010_corrected.csv')\n",
    "\n",
    "display(df.tail())\n",
    "display(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code below\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
