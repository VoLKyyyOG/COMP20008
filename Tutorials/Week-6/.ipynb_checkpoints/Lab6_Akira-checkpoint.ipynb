{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elements Of Data Processing (2021S1) - Week 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping\n",
    "- Web scraping is an automated solution for people who need access to some structured data (i.e table) on a website.\n",
    "- Useful for public websites where an API is not supported to easily get the data or if there is limited access.\n",
    "- `BeautifulSoup` (from `bs4`) is a Python module (similar to `requests` and `urllib`) which help support users to scrape data from web pages for processing and/or website analysis.\n",
    "- The main operations are *scraping* a website for structured data or *crawling* the website by traversing through the index and contents of the website.\n",
    "\n",
    "### Example\n",
    "The example below extracts some tennis scores from the 2019 ATP Tour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import unicodedata\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, use requests to get the html content of the page\n",
    "url = 'https://en.wikipedia.org/wiki/2019_ATP_Tour'\n",
    "\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response code `200` denotes `SUCCESS` (no errors - this is what you want to see).\n",
    "\n",
    "Some other useful ones to know:\n",
    "- `400`: The server (website) has issues or is broken.\n",
    "- `403`: You don't have sufficient permission (i.e authentication required).\n",
    "- `404`: Content doesn't exist.\n",
    "- `405`: Method is not allowed (i.e you used `GET` instead of `POST` or `PUT`).\n",
    "- `500`: Something unexpected happened and the server doesn't know why.\n",
    "- `503`: Server is overloaded and can't handle your current request (but try again later).\n",
    "\n",
    "More here... https://en.wikipedia.org/wiki/List_of_HTTP_status_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the text representation of the html (raw = response.content)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rather than looking at this text, we can use `BeautifulSoup` to parse it!\n",
    "- Since we're looking for the **2019 ATP Tour**, we can use the `.find()` method to find the corresponding section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = BeautifulSoup(response.text, 'html.parser')\n",
    "atp_tag = s.find(id='ATP_ranking')\n",
    "atp_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, let's go to the table (the right table under this headline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = atp_tag.findNext('table').findNext('table')\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple ways to parse this data...\n",
    "- Method 1, parse by directly reading the rows (faster)\n",
    "- Method 2, use pandas (slower, but easier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `unicodedata.normalize()`\n",
    "- Web pages typically use `unicode` encoding, which represents far more characters than your ASCII encoding.\n",
    "- However, since `unicode` has several languages, there can be many ways to express the same characters.\n",
    "- For example, `\"â\"` can be represented as one code point for `\"â\"` (U+00E2), and two decomposed code points for `\"a\"` (U+0061) and `\" ̂\"` (U+0302).\n",
    "- In this specific example, we will use Normalization Form KD (`NFKD`) which decomposes characters by compatibility.\n",
    "- For example: `\"ﬁ\"` (U+FB01) becomes `\"f\"` (U+0066) and `\"i\"` (U+0069)\n",
    "\n",
    "#### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unistr = u'\\u2460'\n",
    "print(f\"{unicodedata.normalize('NFKD', unistr)} is the equivalent character of {unistr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# method 1\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "i = 0\n",
    "records = []\n",
    "    \n",
    "for row in rows[2:]:\n",
    "    cells = row.find_all('td')\n",
    "    record = []\n",
    "    \n",
    "    ranking = int(unicodedata.normalize(\"NFKD\", cells[0].text.strip()))\n",
    "    record.append(int(ranking))\n",
    "    \n",
    "    player = unicodedata.normalize(\"NFKD\", cells[1].text.strip())\n",
    "    # Removes the country from the player name, removing surrounding whitespaces.\n",
    "    player_name = re.sub('\\(.*\\)', '', player).strip()\n",
    "    # print(player_name)\n",
    "    record.append(player_name)\n",
    "\n",
    "    # Remove the thousands separator from the points value and store as an integer\n",
    "    points = unicodedata.normalize(\"NFKD\", cells[2].text.strip())\n",
    "    record.append(int(re.sub(',', '', points)))\n",
    "    \n",
    "    # number of tours: integer type\n",
    "    tours = unicodedata.normalize(\"NFKD\", cells[3].text.strip())\n",
    "    record.append(int(tours))\n",
    "    \n",
    "    # Store the country code separately\n",
    "    country_code = re.search('\\((.*)\\)', player).group(1)\n",
    "    record.append(country_code)\n",
    "\n",
    "    # [1, 'Rafael Nadal', 9585, 12, 'ESP']\n",
    "    records.append(record)\n",
    "    i += 1\n",
    "\n",
    "column_names = [\"Ranking\", \"Player\", \"Points\", \"Tours\", \"Country\"]\n",
    "tennis_data = pd.DataFrame(records, columns = column_names)\n",
    "\n",
    "tennis_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2\n",
    "df = pd.read_html(str(table))[0]\n",
    "display(df.head())\n",
    "\n",
    "# drop the 0th level multi-index (Singles Race Rankings Final rankings[9])\n",
    "# and also drop the nan column\n",
    "# then rename the # to ranking\n",
    "df = df.droplevel(level=0, axis=1).dropna(axis=1).rename({'#': 'Ranking'}, axis=1)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_html(str(table))[0]\n",
    "df = df.droplevel(level=0, axis=1).dropna(axis=1).rename({'#': 'Ranking'}, axis=1)\n",
    "\n",
    "# apply unicode \n",
    "df['Player'] = df['Player'].apply(lambda x: unicodedata.normalize(\"NFKD\", x))\n",
    "\n",
    "# get country value\n",
    "df['Country'] = df['Player'].apply(lambda x: re.search('\\((.*)\\)', x).group(1))\n",
    "\n",
    "# get player text\n",
    "df['Player'] = df['Player'].apply(lambda x: re.sub('\\(.*\\)', '', x).strip())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks(rotation=90)\n",
    "plt.bar(df['Player'], df['Points'])\n",
    "plt.ylabel('Points')\n",
    "plt.title(\"ATP Tour - Player Points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Exercise 1 </span>\n",
    "\n",
    "Produce a graph similar to the example above for the **2019 ATP Doubles Scores**.\n",
    "\n",
    "*First locate the section we're interested in.*\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the page to download\n",
    "url = 'https://en.wikipedia.org/wiki/2019_ATP_Tour'\n",
    "page = requests.get(url)\n",
    "s = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "# add code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web crawling\n",
    "- This is a web crawler that traverses http://books.toscrape.com/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_limit = 20\n",
    "\n",
    "# Specify the initial page to crawl\n",
    "base_url = 'http://books.toscrape.com/'\n",
    "seed_item = 'index.html'\n",
    "\n",
    "seed_url = base_url + seed_item\n",
    "page = requests.get(seed_url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "# initialise dictionary of visit\n",
    "visited = {seed_url: True}\n",
    "pages_visited = 1\n",
    "print(seed_url)\n",
    "\n",
    "# Remove index.html\n",
    "links = soup.findAll('a')\n",
    "seed_link = soup.findAll('a', href=re.compile(\"^index.html\"))\n",
    "to_visit_relative = [l for l in links if l not in seed_link]\n",
    "\n",
    "# Resolve to absolute urls\n",
    "to_visit = []\n",
    "for link in to_visit_relative:\n",
    "    to_visit.append(urljoin(seed_url, link['href']))\n",
    "\n",
    "    \n",
    "# Find all outbound links on succsesor pages and explore each one whilst under visit limit\n",
    "while (to_visit) and pages_visited < page_limit:\n",
    "    # consume the list of urls\n",
    "    link = to_visit.pop(0)\n",
    "    print(link)\n",
    "\n",
    "    # need to concat with base_url, an example item <a href=\"catalogue/sharp-objects_997/index.html\">\n",
    "    page = requests.get(link)\n",
    "    \n",
    "    # scarping code goes here\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # mark the item as visited, i.e., add to visited list, remove from to_visit\n",
    "    visited[link] = True\n",
    "    new_links = soup.findAll('a')\n",
    "    for new_link in new_links :\n",
    "        new_item = new_link['href']\n",
    "        new_url = urljoin(link, new_item)\n",
    "        if new_url not in visited and new_url not in to_visit:\n",
    "            to_visit.append(new_url)\n",
    "        \n",
    "    pages_visited += 1\n",
    "\n",
    "print(f'\\nVisited {len(visited)} pages out of {len(to_visit)} to visit in total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Exercise 2 </span>\n",
    "- The code above can easily be end up stuck in a **crawler trap** (when a crawler crawls an infinite number of irrelevant URLs).  \n",
    "- Explain three ways this could occur and suggest possible solutions\n",
    "- Read more here https://en.wikipedia.org/wiki/Spider_trap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Exercise 3 </span>\n",
    "\n",
    "- Modify the code above to print the titles of as many books as can be found within the `page_limit`.\n",
    "- Only a few additional lines are required where commented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "page_limit = 20\n",
    "\n",
    "# Specify the initial page to crawl\n",
    "base_url = 'http://books.toscrape.com/'\n",
    "seed_item = 'index.html'\n",
    "\n",
    "seed_url = base_url + seed_item\n",
    "page = requests.get(seed_url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "# initialise dictionary of visit\n",
    "visited = {seed_url: True}\n",
    "pages_visited = 1\n",
    "\n",
    "#### initialse an emtpy set or list for book titles here ####\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Remove index.html\n",
    "links = soup.findAll('a')\n",
    "seed_link = soup.findAll('a', href=re.compile(\"^index.html\"))\n",
    "to_visit_relative = [l for l in links if l not in seed_link]\n",
    "\n",
    "# Resolve to absolute urls\n",
    "to_visit = []\n",
    "for link in to_visit_relative:\n",
    "    to_visit.append(urljoin(seed_url, link['href']))\n",
    "    \n",
    "# Find all outbound links on succsesor pages and explore each one whilst under visit limit\n",
    "while (to_visit) and pages_visited < page_limit:\n",
    "    # consume the list of urls\n",
    "    link = to_visit.pop(0)\n",
    "\n",
    "    # need to concat with base_url, an example item <a href=\"catalogue/sharp-objects_997/index.html\">\n",
    "    page = requests.get(link)\n",
    "    \n",
    "    # scarping code goes here\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # mark the item as visited, i.e., add to visited list, remove from to_visit\n",
    "    visited[link] = True\n",
    "    new_links = soup.findAll('a')\n",
    "    \n",
    "    for new_link in new_links:\n",
    "        #### if a new link has attribute 'title', then add new_link['title'] to the set ####\n",
    "\n",
    "        \n",
    "        ####################################################################################\n",
    "        \n",
    "        new_item = new_link['href']\n",
    "        new_url = urljoin(link, new_item)\n",
    "        if new_url not in visited and new_url not in to_visit:\n",
    "            to_visit.append(new_url)\n",
    "        \n",
    "    pages_visited += 1\n",
    "\n",
    "#### print out every title ####\n",
    "\n",
    "\n",
    "###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
